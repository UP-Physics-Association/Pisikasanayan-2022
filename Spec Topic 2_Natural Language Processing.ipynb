{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Pisikasanayan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Application of Machine Learning: Natural Language Processing\n",
        "\n",
        "Natural Language Processing, or NLP, is a special branch of computer sience concerned with letting computers and their programs understand text and spoken words similar to how human beings understand text and spoken words."
      ],
      "metadata": {
        "id": "-tKQDtTsgDOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the Necessary Libraries\n",
        "\n",
        "Here, we again use the `scikit-learn` machine learning libraries, in addition to another open-source and free library specifically for natural language processing: `nltk`. The Natural Language Toolkit, or `nltk`, is a package of different libraries most often used by researchers working with human language data."
      ],
      "metadata": {
        "id": "11IvHEtUggBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtDI4GRtwNrJ",
        "outputId": "f6bc7ce1-f672-4b9d-b3d3-9020f684bb72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ23eJjZepAw"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the Words\n",
        "\n",
        "Here, we go through several different processes that are familiar to NLP.\n",
        "\n",
        "In the function `clean_string`, we pre-process the sentence. We remove the punctuations from the sentence, we put all of the letters in lowercase so the computer has an easier time reading it all, and we remove the most frequent words (e.g. the, an, etc.) which do not contribute as much to the meaning of the sentence.\n",
        "\n",
        "The `clean_vectorizer` function will take care of converting strings to numerical vectors - this function will take care of transforming these sentences, into words, into mathematical objects in a multi-dimensional space.\n",
        "\n",
        "In the function `cosine_sim_vectors`, we measure the cosine similarity between the two strings. Cosine similarity is \"a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.\"\n",
        "\n",
        "Finally, the `compare` function combines all of these functions into one and cleans, vectorizes, and then compares two strings with each other."
      ],
      "metadata": {
        "id": "JUyiT1_rgiqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_ = stopwords.words('english')\n",
        "\n",
        "def clean_string(text):\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    text = text.lower()\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords_])\n",
        "    return text\n",
        "\n",
        "def clean_vectorizer(cleaned_vec):\n",
        "    vectorizer = CountVectorizer().fit_transform(cleaned_vec)\n",
        "    vectors = vectorizer.toarray()\n",
        "    return vectors\n",
        "\n",
        "def cosine_sim_vectors(vec1,vec2):\n",
        "    vec1 = vec1.reshape(1, -1)\n",
        "    vec2 = vec2.reshape(1, -1)\n",
        "    return cosine_similarity(vec1, vec2)[0][0]\n",
        "\n",
        "def compare(a,b):\n",
        "    sentences=[a,b]\n",
        "    cleaned = list(map(clean_string,sentences))\n",
        "    vectors = clean_vectorizer(cleaned)\n",
        "    rawscore = cosine_sim_vectors(vectors[0],vectors[1])\n",
        "    finalscore = rawscore*100\n",
        "    return finalscore"
      ],
      "metadata": {
        "id": "AHY3tnMOey-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare some Sentences\n",
        "\n",
        "Finally, we can compare some similar sentences and see if the program responds well."
      ],
      "metadata": {
        "id": "6l1g080WguZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_sentence = 'This is a foo bar sentence'\n",
        "modified_sentence = 'This sentence is similar to a foo bar sentence.'"
      ],
      "metadata": {
        "id": "CgvGBZEBfySS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The two sentences are {:.2f}% similar.'.format(compare(original_sentence, modified_sentence)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc4ywh7dwaFX",
        "outputId": "8b00d4ca-f7e3-4553-a375-ff678152f776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The two sentences are 87.29% similar.\n"
          ]
        }
      ]
    }
  ]
}